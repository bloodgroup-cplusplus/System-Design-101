---
slug: distributed-databases
title: Distributed Databases
readTime: 20 min
orderIndex: 4
premium: false
---




Distributed Databases: When One Server Isn't Enough (Google's Spanner Revolution)
ðŸŽ¯ Challenge 1: The Planetary Library Problem
Imagine this scenario: You're building a library system that needs to serve the entire planet - billions of users, petabytes of data, 24/7 availability.

Traditional Single Database (Centralized):
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Library in New York              â”‚
â”‚    - All books stored here          â”‚
â”‚    - Single building                â”‚
â”‚    - One massive catalog            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†‘          â†‘          â†‘
    User in NY  User in     User in
    (fast!)     London      Tokyo
                (slow!)     (very slow!)

Problems:
âŒ Users in Asia wait 200ms+ (latency)
âŒ Single point of failure (building burns = all data lost)
âŒ Can't handle billions of users (capacity limit)
âŒ Expensive to scale vertically (bigger building)
âŒ Backup takes hours (too much data)
```

Distributed Database (Decentralized):
```
New York          London           Tokyo          SÃ£o Paulo
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Library   â”‚â†â”€â”€â†’â”‚ Library   â”‚â†â”€â”€â†’â”‚ Library   â”‚â†â”€â”€â†’â”‚ Library   â”‚
â”‚ Shard 1   â”‚    â”‚ Shard 2   â”‚    â”‚ Shard 3   â”‚    â”‚ Shard 4   â”‚
â”‚ Books A-F â”‚    â”‚ Books G-M â”‚    â”‚ Books N-S â”‚    â”‚ Books T-Z â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â†‘                â†‘                â†‘                â†‘
   Users in        Users in         Users in         Users in
   Americas        Europe           Asia             S. America
   (5ms!)          (5ms!)           (5ms!)           (5ms!)

Coordination:
- Each library has part of the collection (sharding)
- Each library has copies of some books (replication)
- Libraries coordinate to maintain consistency
- Router directs you to right library

Benefits:
âœ… Low latency worldwide (serve from nearest location)
âœ… Massive scalability (billions of users, petabytes of data)
âœ… High availability (one library down, others continue)
âœ… No single point of failure (distributed across globe)
âœ… Easier to scale horizontally (add more libraries)
```

Pause and think: What if your database could be spread across multiple servers, multiple datacenters, even multiple continents, working as one unified system?

The Answer: Distributed databases split data and processing across multiple nodes while appearing as a single system! It's like:
âœ… Data partitioned across multiple servers (horizontal scaling)
âœ… Each partition replicated for availability (fault tolerance)
âœ… Nodes coordinate using consensus algorithms (consistency)
âœ… Queries route to appropriate nodes automatically (transparency)
âœ… Scales to planetary size (Google, Amazon, Facebook)

Key Insight: Distributed databases trade simplicity for massive scale and global availability!

ðŸŽ¬ Interactive Exercise: Single vs Distributed Database

Single Database (Monolithic):
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     PostgreSQL on Big Server        â”‚
â”‚     - 1TB RAM                       â”‚
â”‚     - 64 CPU cores                  â”‚
â”‚     - 10TB SSD                      â”‚
â”‚     - Handles 10,000 queries/sec    â”‚
â”‚     - Cost: $50,000/month           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Scaling:
- Year 1: Works great! âœ“
- Year 2: Need more RAM â†’ Upgrade to 2TB ($70k/month)
- Year 3: Need more CPU â†’ Upgrade to 128 cores ($100k/month)
- Year 4: Hit hardware limits! âŒ Can't scale further

Characteristics:
âœ… Simple (one server, ACID transactions)
âœ… Consistent (no distributed coordination)
âŒ Limited scalability (hardware ceiling)
âŒ Single point of failure
âŒ Expensive at scale
```

Distributed Database (Horizontal):
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Node 1  â”‚ â”‚  Node 2  â”‚ â”‚  Node 3  â”‚ â”‚  Node 4  â”‚ â”‚  Node 5  â”‚
â”‚ 32GB RAM â”‚ â”‚ 32GB RAM â”‚ â”‚ 32GB RAM â”‚ â”‚ 32GB RAM â”‚ â”‚ 32GB RAM â”‚
â”‚ 8 cores  â”‚ â”‚ 8 cores  â”‚ â”‚ 8 cores  â”‚ â”‚ 8 cores  â”‚ â”‚ 8 cores  â”‚
â”‚ 1TB SSD  â”‚ â”‚ 1TB SSD  â”‚ â”‚ 1TB SSD  â”‚ â”‚ 1TB SSD  â”‚ â”‚ 1TB SSD  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Each handles 2,000 queries/sec = 10,000 total
Cost: $5,000/month each = $25,000/month total (cheaper!)

Scaling:
- Year 1: 5 nodes âœ“
- Year 2: Add 5 more nodes â†’ 10 nodes total
- Year 3: Add 10 more nodes â†’ 20 nodes total
- Year 4: Add 20 more nodes â†’ 40 nodes total

Unlimited growth! Just add more commodity servers!

Characteristics:
âœ… Unlimited scalability (add more nodes)
âœ… High availability (nodes fail independently)
âœ… Cost effective (commodity hardware)
âŒ Complex (distributed coordination)
âŒ Eventual consistency (or coordination overhead)
```

The Trade-off:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Aspect        â”‚ Single DB      â”‚ Distributed DB       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Scalability   â”‚ Vertical (â†‘)   â”‚ Horizontal (â†’)       â”‚
â”‚ Cost at scale â”‚ Very expensive â”‚ Cost effective       â”‚
â”‚ Consistency   â”‚ Strong (ACID)  â”‚ Eventual/Tunable     â”‚
â”‚ Complexity    â”‚ Simple         â”‚ Complex              â”‚
â”‚ Latency       â”‚ Low (local)    â”‚ Variable (network)   â”‚
â”‚ Availability  â”‚ Single point   â”‚ No single point      â”‚
â”‚ Max size      â”‚ ~10TB          â”‚ Unlimited (petabytes)â”‚
â”‚ Transactions  â”‚ Easy           â”‚ Hard (distributed)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Real-world parallel: Single database is like a skyscraper (limited height, expensive). Distributed database is like a city (unlimited growth, add more buildings).

ðŸ—ï¸ Types of Distributed Databases

Type 1: Distributed SQL (NewSQL)
```
Examples: Google Spanner, CockroachDB, YugabyteDB

Architecture:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              SQL Query Interface                       â”‚
â”‚         (Looks like PostgreSQL/MySQL)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚              â”‚              â”‚
        â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
        â”‚ Region 1â”‚    â”‚ Region 2â”‚    â”‚ Region 3â”‚
        â”‚ US-East â”‚    â”‚ EU-West â”‚    â”‚ Asia    â”‚
        â”‚ 3 nodes â”‚    â”‚ 3 nodes â”‚    â”‚ 3 nodes â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Features:
âœ… ACID transactions (even across nodes!)
âœ… SQL interface (familiar)
âœ… Horizontal scalability
âœ… Strong consistency
âœ… Multi-region deployment

Use case:
- Global applications needing ACID
- Migration from traditional RDBMS
- Financial systems at scale

Trade-off:
âš ï¸ Slower than single-node (coordination overhead)
âš ï¸ More expensive than NoSQL
```

Type 2: Eventually Consistent NoSQL
```
Examples: Cassandra, DynamoDB, Riak

Architecture:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Node 1  â”‚ â”‚  Node 2  â”‚ â”‚  Node 3  â”‚ â”‚  Node 4  â”‚
â”‚  (Equal) â”‚ â”‚  (Equal) â”‚ â”‚  (Equal) â”‚ â”‚  (Equal) â”‚
â”‚   A-F    â”‚ â”‚   G-M    â”‚ â”‚   N-S    â”‚ â”‚   T-Z    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â†•            â†•            â†•            â†•
All nodes equal (no master!)
Data replicated across nodes
Write to any node

Features:
âœ… Extreme scalability (linear)
âœ… High availability (no single point)
âœ… Low latency (write anywhere)
âœ… Tunable consistency
âœ… Multi-datacenter

Use case:
- Massive scale (billions of records)
- High write throughput
- Eventual consistency acceptable

Trade-off:
âŒ No ACID transactions
âŒ Eventual consistency
âŒ No JOINs
```

Type 3: Sharded Traditional DB
```
Example: Vitess (MySQL), Citus (PostgreSQL)

Architecture:
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Query Router  â”‚
         â”‚    (Vitess)    â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â†“             â†“             â†“             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚MySQL    â”‚  â”‚MySQL    â”‚  â”‚MySQL    â”‚  â”‚MySQL    â”‚
â”‚Shard 1  â”‚  â”‚Shard 2  â”‚  â”‚Shard 3  â”‚  â”‚Shard 4  â”‚
â”‚Users    â”‚  â”‚Users    â”‚  â”‚Users    â”‚  â”‚Users    â”‚
â”‚1-25M    â”‚  â”‚25M-50M  â”‚  â”‚50M-75M  â”‚  â”‚75M-100M â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Features:
âœ… Familiar MySQL/PostgreSQL
âœ… Scales horizontally
âœ… Can keep existing app code
âœ… ACID within shard

Use case:
- Scaling existing MySQL/PostgreSQL
- Need SQL and horizontal scale

Trade-off:
âŒ Cross-shard queries slow
âŒ Cross-shard transactions hard
âŒ Resharding complexity
```

Type 4: Distributed Document Stores
```
Example: MongoDB (sharded cluster)

Architecture:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Mongos (Query Router)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
         â”‚          â”‚          â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â” â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â” â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”
    â”‚Shard 1 â”‚ â”‚Shard 2 â”‚ â”‚Shard 3 â”‚
    â”‚Replica â”‚ â”‚Replica â”‚ â”‚Replica â”‚
    â”‚  Set   â”‚ â”‚  Set   â”‚ â”‚  Set   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Features:
âœ… Flexible schema (documents)
âœ… Horizontal scaling (sharding)
âœ… High availability (replica sets)
âœ… Rich query language

Use case:
- Document-oriented data
- Flexible schema needs
- Rapid development

Trade-off:
âŒ Eventual consistency (default)
âŒ Limited transactions (improving)
```

Real-world parallel:
- Distributed SQL = National chain with strict policies (consistent everywhere)
- NoSQL = Franchises with local autonomy (eventual sync)
- Sharded traditional = Multiple branches of same bank
- Document stores = Flexible filing system across locations

ðŸŽ® Decision Game: Which Distributed Database?

Context: You're choosing a database for different use cases.

Scenarios:
A. Global e-commerce platform (need ACID for orders)
B. Social media feed (billions of posts, eventual consistency OK)
C. Real-time analytics (massive data ingestion)
D. Financial trading system (strong consistency critical)
E. IoT sensor data (millions of devices)
F. Multi-tenant SaaS (need isolation)
G. Content management system (flexible schema)
H. Gaming leaderboard (extremely high writes)

Options:
1. Distributed SQL (CockroachDB/Spanner)
2. Eventually Consistent NoSQL (Cassandra/DynamoDB)
3. Sharded PostgreSQL (Citus)
4. MongoDB Sharded Cluster

Answers:
```
A. Global e-commerce â†’ Distributed SQL (1)
   Reason: Need ACID for orders, global scale

B. Social media feed â†’ NoSQL (2)
   Reason: Massive scale, eventual consistency fine

C. Real-time analytics â†’ NoSQL (2)
   Reason: High write throughput needed

D. Financial trading â†’ Distributed SQL (1)
   Reason: Strong consistency critical

E. IoT sensor data â†’ NoSQL (2)
   Reason: Billions of writes, time-series

F. Multi-tenant SaaS â†’ Sharded PostgreSQL (3)
   Reason: Need SQL, isolation per tenant

G. Content management â†’ MongoDB (4)
   Reason: Flexible schema, rich queries

H. Gaming leaderboard â†’ NoSQL (2)
   Reason: Extreme write throughput
```

ðŸš¨ Common Misconception: "Distributed = Eventually Consistent... Right?"

You might think: "All distributed databases sacrifice consistency."

The Reality: Modern distributed databases offer strong consistency!

Understanding Consistency Models:

Eventual Consistency:
```
Time: 10:00:00
Node 1 (US): Write X=5
Node 2 (EU): Still has X=0

Time: 10:00:01 (1 second later)
Node 1 (US): X=5
Node 2 (EU): X=5 (synced!)

During that 1 second:
- Read from Node 1: X=5 âœ“
- Read from Node 2: X=0 âœ— (stale!)

Eventually consistent = Will be consistent eventually
```

Strong Consistency (Linearizability):
```
Time: 10:00:00
Node 1 (US): Write X=5
  â†“
Consensus protocol (Raft/Paxos)
  â†“
Wait for majority of nodes to confirm
  â†“
Node 1: X=5 âœ“
Node 2: X=5 âœ“
Node 3: X=5 âœ“
  â†“
Return success to client

Now reading from ANY node: X=5 âœ“

Strongly consistent = Always see latest value
```

Google Spanner Example:
```sql
-- Spanner provides ACID across continents!

-- Create table (automatically distributed)
CREATE TABLE users (
  user_id INT64 NOT NULL,
  email STRING(256),
  balance NUMERIC
) PRIMARY KEY (user_id);

-- ACID transaction across multiple rows/nodes
BEGIN TRANSACTION;
  UPDATE users SET balance = balance - 100 WHERE user_id = 123;
  UPDATE users SET balance = balance + 100 WHERE user_id = 456;
COMMIT;

-- Even if user 123 and 456 are on different continents!
-- Spanner guarantees ACID using:
-- 1. Two-phase commit
-- 2. TrueTime (atomic clocks for ordering)
-- 3. Paxos for consensus

Result: Distributed AND strongly consistent! âœ“
```

How It's Possible:
```
Spanner's TrueTime:
- Atomic clocks in every datacenter
- GPS-synchronized time
- Knows uncertainty bound (Â±7ms)
- Uses time to order transactions globally

Example:
Transaction 1: timestamp = 10:00:00.003 Â± 0.007
Transaction 2: timestamp = 10:00:00.020 Â± 0.007

Spanner knows T2 happened after T1 (no overlap)
Can safely order them!

Trade-off:
âœ… Strong consistency globally
âŒ Higher latency (coordination)
âŒ More expensive (atomic clocks, coordination)
```

Real-world parallel: Strong consistency is like a global conference call (everyone hears same thing, but takes time to coordinate). Eventual consistency is like email (everyone gets message, but at different times).

âš¡ Distributed Consensus: How Nodes Agree

The Challenge:
```
3 nodes need to agree on a value:

Node 1 proposes: X=5
Node 2 proposes: X=10
Node 3 proposes: X=15

How do they agree on ONE value?
What if Node 2 is down?
What if network is slow?
```

Raft Consensus Algorithm:
```
Step 1: Leader Election
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Node 1  â”‚    â”‚ Node 2  â”‚    â”‚ Node 3  â”‚
â”‚Candidateâ”‚    â”‚Candidateâ”‚    â”‚Candidateâ”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚              â”‚              â”‚
     â”œâ”€ Vote for me! â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’
     â”‚              â”‚              â”‚
     â†â”€â”€â”€ Vote for Node 1 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
     â”‚              â”‚              â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€ Becomes Leader! â”€â”€â”€â”€â”˜

Step 2: Log Replication
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Leader  â”‚    â”‚Follower â”‚    â”‚Follower â”‚
â”‚ Node 1  â”‚    â”‚ Node 2  â”‚    â”‚ Node 3  â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚              â”‚              â”‚
     â”œâ”€ Replicate X=5 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’
     â”‚              â”‚              â”‚
     â†â”€â”€â”€ ACK â”€â”€â”€â”€â”€â”€â”¤              â”‚
     â”‚              â†â”€â”€â”€â”€â”€ ACK â”€â”€â”€â”€â”¤
     â”‚              â”‚              â”‚
     â””â”€â”€ Commit (majority confirmed!)

Step 3: Client Gets Response
Client: "SET X=5"
  â†“
Leader: "Replicated to majority, committed!" âœ“
  â†“
Client: "Success!"

Key: Need majority (quorum) to commit
3 nodes: Need 2 to agree
5 nodes: Need 3 to agree
```

Handling Failures:
```
Scenario: Leader fails during replication

Before:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Leader  â”‚    â”‚Follower â”‚    â”‚Follower â”‚
â”‚ Node 1  â”‚    â”‚ Node 2  â”‚    â”‚ Node 3  â”‚
â”‚ Log: X=5â”‚    â”‚ Log: X=5â”‚    â”‚ Log: X=5â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
   ðŸ’¥ Crashes!

After:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Leader  â”‚    â”‚Follower â”‚
â”‚ Node 2  â”‚â†â”€â”€â”€â”‚ Node 3  â”‚
â”‚ Log: X=5â”‚    â”‚ Log: X=5â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

New election!
Node 2 becomes leader (has all committed data)
System continues! âœ“

No data lost! (X=5 was committed to majority)
```

CockroachDB Example:
```sql
-- CockroachDB uses Raft for consensus

-- Create table (automatically replicated with Raft)
CREATE TABLE accounts (
  id INT PRIMARY KEY,
  balance DECIMAL
);

-- Insert (replicated via Raft)
INSERT INTO accounts VALUES (1, 1000);

Behind the scenes:
1. Leader receives INSERT
2. Proposes to followers via Raft
3. Waits for majority (quorum)
4. Commits when majority confirms
5. Returns success to client

-- Even if one node fails:
-- System continues (has quorum)
-- Data is safe (majority has it)

-- Check replication status
SHOW RANGES FROM TABLE accounts;

 range_id | replicas | lease_holder
----------+----------+--------------
    42    | {1,2,3}  |      1
```

Real-world parallel: Raft is like a committee vote. Majority must agree before decision is official. If some members absent, majority of present members still sufficient.

ðŸ”§ Distributed Transactions: The Hard Problem

The Two-Phase Commit (2PC):
```
Scenario: Transfer money across 2 databases

Phase 1: Prepare
Coordinator: "Can you commit this transaction?"
   â†“
Database 1 (US): "Yes, I can" âœ“
Database 2 (EU): "Yes, I can" âœ“
   â†“
All voted YES? Proceed to phase 2

Phase 2: Commit
Coordinator: "Everyone commit!"
   â†“
Database 1: Commits âœ“
Database 2: Commits âœ“
   â†“
Coordinator: "Transaction complete!" âœ“

If anyone votes NO:
Coordinator: "Everyone rollback!"
All databases rollback
Transaction aborted
```

The Problem with 2PC:
```
Scenario: Coordinator crashes after "prepare" but before "commit"

Phase 1: Prepare
Coordinator: "Can you commit?"
Database 1: "Yes" (waiting...)
Database 2: "Yes" (waiting...)
   â†“
Coordinator crashes! ðŸ’¥
   â†“
Databases stuck waiting!
Can't commit (no confirmation)
Can't rollback (might have been commit)

BLOCKED! This is why 2PC is problematic in practice.
```

Modern Solutions:

Saga Pattern:
```
Instead of locking, use compensating transactions:

Step 1: Debit Account A (succeeds)
Step 2: Credit Account B (fails!)
Step 3: Compensate: Credit Account A back

Series of local transactions
Each can be reversed
No distributed locking!

Use case: Microservices, long-running transactions
```

Spanner's Solution:
```
Uses TrueTime + Paxos:

1. Assign globally unique timestamp
2. Use Paxos for consensus (better than 2PC)
3. Wait out uncertainty (ensure timestamp is globally unique)
4. Commit

Result: True distributed ACID transactions! âœ“
Cost: Higher latency (coordination + wait)
```

Real-world parallel: 2PC is like getting signatures from multiple people. If courier (coordinator) lost in transit, everyone waits. Saga is like a reversible process - can undo if something fails.

ðŸ’¡ Sharding in Distributed Databases

Automatic Sharding (CockroachDB):
```
-- Create table
CREATE TABLE users (
  id UUID PRIMARY KEY,
  name STRING,
  email STRING
);

-- CockroachDB automatically:
-- 1. Splits data into ranges (64MB each)
-- 2. Distributes ranges across nodes
-- 3. Replicates each range (3 copies default)
-- 4. Rebalances automatically

-- Check how data is distributed
SHOW RANGES FROM TABLE users;

range_id | start_key | end_key | replicas | lease_holder
---------+-----------+---------+----------+-------------
   100   | /1        | /1000   | {1,2,3}  |      1
   101   | /1000     | /2000   | {2,3,4}  |      2
   102   | /2000     | /3000   | {3,4,5}  |      3

-- Application doesn't need to know about sharding!
-- Just query normally:
SELECT * FROM users WHERE id = '...';
-- CockroachDB routes to correct range automatically
```

Manual Sharding (MongoDB):
```javascript
// Enable sharding on database
sh.enableSharding("myapp")

// Shard collection by user_id (hash-based)
sh.shardCollection("myapp.users", { user_id: "hashed" })

// MongoDB automatically:
// - Distributes data across shards
// - Routes queries to correct shard
// - Balances data across shards

// Check shard distribution
db.users.getShardDistribution()

// Output:
// Shard shard1: 2500000 docs, 50% of data
// Shard shard2: 2500000 docs, 50% of data
```

Real-world parallel: Automatic sharding is like a valet parking service (handles distribution automatically). Manual sharding is like parking lot sections (you decide where to park).

ðŸŒ Multi-Region Deployment Patterns

Pattern 1: Primary in One Region (Read Replicas Everywhere)
```
       US-East (Primary)           Europe (Replica)         Asia (Replica)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Read/Write           â”‚â”€â”€â”€â”€â”€â†’â”‚ Read Only        â”‚â”€â”€â”€â†’â”‚ Read Only        â”‚
â”‚ 3 nodes (Raft)       â”‚      â”‚ 3 nodes          â”‚    â”‚ 3 nodes          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Characteristics:
âœ… Strong consistency (single primary)
âœ… Fast reads globally (local replicas)
âŒ Slow writes globally (must go to primary)

Use case: Read-heavy workload, writes can tolerate latency
```

Pattern 2: Regional Primaries (Multi-Region Primary-Primary)
```
       US-East                    Europe                    Asia
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Primary          â”‚â†â”€â”€â”€â”€â†’â”‚ Primary          â”‚â†â”€â”€â†’â”‚ Primary          â”‚
â”‚ Data: US users   â”‚      â”‚ Data: EU users   â”‚    â”‚ Data: APAC users â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Characteristics:
âœ… Fast writes locally (write to local primary)
âœ… Fast reads locally (read from local)
âš ï¸ Cross-region transactions slow
âš ï¸ Eventual consistency across regions

Use case: Geo-partitioned data (US users in US, EU users in EU)
```

Pattern 3: Global Consensus (Spanner-style)
```
       US-East                    Europe                    Asia
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3 nodes          â”‚â†â”€â”€â”€â”€â†’â”‚ 3 nodes          â”‚â†â”€â”€â†’â”‚ 3 nodes          â”‚
â”‚ Participate in   â”‚      â”‚ Participate in   â”‚    â”‚ Participate in   â”‚
â”‚ global consensus â”‚      â”‚ global consensus â”‚    â”‚ global consensus â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

All regions participate in Paxos/Raft
Any region can initiate transactions
Strong consistency globally

Characteristics:
âœ… Strong consistency everywhere
âœ… ACID transactions globally
âŒ Higher latency (global consensus)

Use case: Financial systems, need global ACID
```

Real-world parallel:
- Pattern 1 = Headquarters with branch offices
- Pattern 2 = Regional headquarters (autonomous)
- Pattern 3 = Federation (all regions vote on decisions)

ðŸ’¡ Final Synthesis Challenge: The Global Corporation

Complete this comparison:
"A single database is like a company in one building. A distributed database is like..."

Your answer should include:
- Data distribution
- Consistency guarantees
- Scalability approach
- Global availability

Take a moment to formulate your complete answer...

The Complete Picture:
A distributed database is like a multinational corporation with offices worldwide:

âœ… **Headquarters + Branches** (Primary-Replica): Central office makes decisions, branches execute locally
âœ… **Regional Autonomy** (Multi-Primary): Each region operates independently, syncs periodically
âœ… **Federation** (Consensus): All offices vote on major decisions (Raft/Paxos)
âœ… **Departments** (Sharding): Each office handles specific customer segments
âœ… **Redundancy** (Replication): Multiple offices have same information
âœ… **Global Scale**: Can serve billions worldwide from nearest location
âœ… **Coordination**: Offices communicate to maintain consistency

**Benefits:**
1. **Unlimited Scale** - Add more nodes = more capacity (horizontal)
2. **Global Availability** - Serve users from nearest datacenter (low latency)
3. **Fault Tolerance** - One datacenter fails, others continue
4. **Geo-Distribution** - Data in multiple locations (compliance, disaster recovery)

**Trade-offs:**
- Complexity (distributed coordination)
- Consistency challenges (or latency cost)
- More expensive operations (network calls)
- Harder to reason about (distributed state)

**Real-world examples:**
- **Spanner** (Google): Global ACID transactions for AdWords
- **DynamoDB** (Amazon): Massive scale for e-commerce
- **Cassandra** (Netflix): Billions of viewing events
- **CockroachDB**: Cloud-native distributed SQL
- **MongoDB Atlas**: Global document database

**When to use:**
- Data > 1TB and growing fast
- Need global low latency
- Single server can't handle load
- High availability critical
- Can accept complexity trade-off

**When NOT to use:**
- Data < 100GB
- Single region sufficient
- Team lacks distributed systems expertise
- Simple application

Distributed databases transform single-server limits into planetary-scale systems!

ðŸŽ¯ Quick Recap: Test Your Understanding
Without looking back, can you explain:
1. How do distributed databases differ from sharded traditional databases?
2. What is Raft consensus and why is it needed?
3. How does Google Spanner achieve strong consistency globally?
4. When should you choose distributed SQL vs NoSQL?

Mental check: If you can design a distributed database architecture, you understand distributed databases!

ðŸš€ Your Next Learning Adventure
Now that you understand distributed databases, explore:

Advanced Topics:
- Vector clocks and conflict resolution
- CRDTs (Conflict-Free Replicated Data Types)
- Distributed tracing and observability
- Jepsen testing (distributed systems testing)

Distributed Databases:
- CockroachDB deep dive
- YugabyteDB architecture
- TiDB (distributed MySQL)
- FoundationDB (key-value store)

Consensus Algorithms:
- Raft visualization
- Paxos algorithm
- Byzantine fault tolerance
- Blockchain consensus

Real-World Case Studies:
- How Uber built Schemaless
- Discord's database scaling
- Dropbox's migration to distributed storage
- How Stripe handles distributed transactions
